A veces más no es mejor. En un post encontré (https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm) que por lo general 1 capa de neuronas LSTM es suficiente para capturar comportamientos simples y 2 para comportamientos más complejos. Entonces dije: Porque no? E intenté agregarle una segunda capa a la red. Definitivamente no funcionó. Las partículas se disparaban a las esquinas y comenzaban a hacer patrones extraños. Peculiar y chistoso a veces, pero no muy útil. No estoy seguro si esto se debe a under-fitting, over-fitting o simplemente un entrenamiento con malos hiperparámetros. El RMSE y el loss eran muy muy bajos, pero los resultados no eran los deseados, entonces... lo que se me imagina es que hubo under-fitting o en otras palabras un modelo muy grande para data muy simple. De nuevo la BiLSTM es la que más promesa presentó, pero las otras si eran un caso perdido. Si la BiLSTM hacía cosas raras, la LSTM normal y la GRU hacían nada, solo popó. 

Definitivamente no se deben incrementar tanto el tamaño de las capas para el tamaño de input y output que se está manejando ahorita.